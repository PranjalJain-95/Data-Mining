{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4e0033-ee59-40df-84ad-5a159deb421a",
   "metadata": {},
   "source": [
    "## Exam II: CSCI4390-6390 (100 points)\n",
    "\n",
    "This is a take-home exam. It is due 11:59:59pm on Thursday, Nov 3 via submitty. You are expected to abide by the honor code, i.e., all work must be your own, and you are not allowed to discuss any aspect of exam with anyone except the TA or the professor. You are also not allowed to use the internet except to lookup numpy/python documentation, the book site, class notes/videos, or to ask clarification questions on campuswire. **You cannot use any other library other than numpy (e.g., no pytorch, sklearn, etc); you can use pandas only for data processing**. You must sign the declaration below that you will abide by the honor code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78163f5-20e5-427d-9d62-977d2ee7a1e6",
   "metadata": {},
   "source": [
    "#### **Honor Code Declaration**: Please sign with your name to acknowledge that you agree to abide by the honor code, and demonstrate the highest level of academic integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92536081-83a0-4b65-b3e7-557e384915db",
   "metadata": {},
   "source": [
    "Pranjal Jain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c78be-4a74-44c1-818f-8f5bb77fc69c",
   "metadata": {},
   "source": [
    "#### Download Dataset: Download the [Seeds Dataset](https://archive.ics.uci.edu/ml/datasets/seeds). It has 9 attributes, and 210 points. The last attribute is the class variable. Store the dataset in your current directory and use \"./seeds_dataset.txt\" as its name. **DO NOT** submit the dataset as part of your answer on submitty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ce8a96-6615-4543-a445-a53b8ca6922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('./seeds_dataset.txt', sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2707fff6-bbc9-4b29-ac74-c122b6b91433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          0      1       2      3      4      5      6  7\n",
       "0    15.26  14.84  0.8710  5.763  3.312  2.221  5.220  1\n",
       "1    14.88  14.57  0.8811  5.554  3.333  1.018  4.956  1\n",
       "2    14.29  14.09  0.9050  5.291  3.337  2.699  4.825  1\n",
       "3    13.84  13.94  0.8955  5.324  3.379  2.259  4.805  1\n",
       "4    16.14  14.99  0.9034  5.658  3.562  1.355  5.175  1\n",
       "..     ...    ...     ...    ...    ...    ...    ... ..\n",
       "205  12.19  13.20  0.8783  5.137  2.981  3.631  4.870  3\n",
       "206  11.23  12.88  0.8511  5.140  2.795  4.325  5.003  3\n",
       "207  13.20  13.66  0.8883  5.236  3.232  8.315  5.056  3\n",
       "208  11.84  13.21  0.8521  5.175  2.836  3.598  5.044  3\n",
       "209  12.30  13.34  0.8684  5.243  2.974  5.637  5.063  3\n",
       "\n",
       "[210 rows x 8 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4632d-993f-4479-bbd5-2996fab7aa40",
   "metadata": {},
   "source": [
    "#### Q1. Linear Regression: (Total 25 points)  For this question, we will use the first attribute as the target, and you will ignore the last attribute, which is the class. We will use linear regression to predict the target via the geometric approach. Answer the following:\n",
    "\n",
    "##### a) (10 points) First find and print the orthogonal basis vectors (without unit normalization) for the independent attributes; print the first three and last three values of each basis vector. You cannot call numpy QR or simialr function for this.\n",
    "\n",
    "##### b) (5 points) Next find and print the predicted vector, and show the squared error. You may not use numpy QR or similar function for this problem. \n",
    "\n",
    "##### c) (5 points) Find and print the regression coefficient or weight vector, using any method of choice. You can use the numpy inv function.\n",
    "\n",
    "##### d) (5 points) Plot the scatter plot and regression line for the target versus the first attribute. Note: do not center the data, and do not run a separate regression for this. You must plot the effect of 1st attribute based on the full multivariate regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b5efdc-55de-4555-9198-e39c4b8bfe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df.columns[0:7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bc52a98-766c-4df2-953f-6ab2648667fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df1[df1.columns[1:7]].to_numpy()\n",
    "X_train = np.append(np.ones((X_train.shape[0], 1)), X_train, axis=1)\n",
    "Y = df1[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ed4562e-63bc-4eb5-a333-76d1bfd5ae64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  2.80714286e-01 -2.68666067e-03 ... -2.16672272e-03\n",
      "  -1.42209631e+00 -2.62296882e-01]\n",
      " [ 1.00000000e+00  1.07142857e-02  9.99882975e-03 ...  6.39689071e-03\n",
      "  -2.56912585e+00 -2.41113780e-01]\n",
      " [ 1.00000000e+00 -4.69285714e-01  3.84952572e-02 ... -2.32061234e-02\n",
      "  -1.96882473e-01 -1.18489995e-01]\n",
      " ...\n",
      " [ 1.00000000e+00 -8.99285714e-01  2.59128900e-02 ...  6.91857121e-02\n",
      "   4.27131323e+00 -5.72193143e-02]\n",
      " [ 1.00000000e+00 -1.34928571e+00 -5.97795927e-03 ... -2.46484383e-02\n",
      "  -3.53448753e-01  8.47947022e-02]\n",
      " [ 1.00000000e+00 -1.21928571e+00  9.07717498e-03 ...  4.88237671e-03\n",
      "   1.79274780e+00  9.73647506e-03]]\n",
      "U 0 :  [1. 1. 1.] [1. 1. 1.]\n",
      "U 1 :  [ 0.28071429  0.01071429 -0.46928571] [-0.89928571 -1.34928571 -1.21928571]\n",
      "U 2 :  [-0.00268666  0.00999883  0.03849526] [ 0.02591289 -0.00597796  0.00907717]\n",
      "U 3 :  [ 0.03158919 -0.03985449 -0.03559144] [ 0.00318121 -0.03124122  0.05140863]\n",
      "U 4 :  [-0.00216672  0.00639689 -0.02320612] [ 0.06918571 -0.02464844  0.00488238]\n",
      "U 5 :  [-1.42209631 -2.56912585 -0.19688247] [ 4.27131323 -0.35344875  1.7927478 ]\n",
      "U 6 :  [-0.26229688 -0.24111378 -0.11848999] [-0.05721931  0.0847947   0.00973648]\n"
     ]
    }
   ],
   "source": [
    "def orth_proj_uj(uj,ai):\n",
    "    return (np.dot(uj,ai) / np.dot(ai,ai))\n",
    "\n",
    "def sub_proj_qr(X):\n",
    "    n, d = np.shape(X)\n",
    "    Q = np.zeros((n, d))\n",
    "    R = np.zeros((d, d))\n",
    "    np.fill_diagonal(R, 1)\n",
    "    #mat1 = np.zeros(mat.shape) \n",
    "    Q[:,0] = X[:,0]\n",
    "    for i in range(1,d):\n",
    "        ui = X[:,i]\n",
    "        for j in range(i):\n",
    "            R[j,i] = orth_proj_uj(X[:,i], Q[:,j])\n",
    "            ui = ui - R[j,i] * Q[:,j]\n",
    "        Q[:,i] = ui\n",
    "    return(Q,R)\n",
    "\n",
    "q,r = sub_proj_qr(X_train)\n",
    "print(q)\n",
    "for i in range(q.shape[1]):\n",
    "    print('U',i,': ', q[:3,i],q[207:,i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "277bb334-3cfe-4d30-85e7-bc3b0d50a247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predected values of Y:  [15.32249901 14.89996759 14.28791616 13.94780753 16.15700819 14.40484421\n",
      " 14.76543074 14.16696301 16.782528   16.49603943 15.3059898  14.0536285\n",
      " 13.81955142 13.75706021 13.66834787 14.5542839  14.021522   15.68281446\n",
      " 14.68396759 12.74707833 14.23216309 14.18689002 15.87207043 12.02138733\n",
      " 14.98952462 16.15523911 12.97009388 12.67137881 14.13787683 13.50458976\n",
      " 13.10086841 15.58175681 14.28181459 13.94157033 15.16892055 16.14661433\n",
      " 16.35185121 17.09983594 14.88534304 14.34690054 13.59974955 13.5792973\n",
      " 13.15054448 15.64392553 15.22425552 13.78678589 15.38746862 15.07214259\n",
      " 14.83520526 14.97966148 14.54334067 15.78626949 14.61195593 14.37140563\n",
      " 14.62416523 15.11996331 14.60852679 14.98451446 15.46079437 12.12299234\n",
      " 11.2056282  11.02078936 12.22216733 13.27895274 12.69709675 12.79530394\n",
      " 14.36875458 14.09220838 14.42272503 12.69873033 17.78098348 17.06997711\n",
      " 17.40206237 19.05824456 16.93613321 16.93699068 17.40133794 20.43385037\n",
      " 18.91725872 17.17786134 16.68711332 18.72556413 19.96386533 19.48361374\n",
      " 19.44156208 18.30190157 18.79609849 19.10847155 20.77973933 20.571995\n",
      " 19.94370278 18.7146675  18.68751602 18.56808691 18.51743979 17.04560733\n",
      " 19.2921226  18.92534842 18.24904952 18.68360671 16.55709006 17.97776226\n",
      " 19.34534426 19.13429664 18.93225159 18.75808095 18.78966329 17.68502517\n",
      " 19.82960598 18.45953434 18.40530218 19.32884956 19.01979072 19.13366929\n",
      " 20.69862772 18.95711675 18.83538603 19.11585286 18.73800816 19.89434453\n",
      " 20.09615713 18.13485543 16.39401613 18.35080277 16.06245922 18.75752601\n",
      " 18.61406886 17.9695522  19.92109509 17.58936401 18.24043511 18.88517965\n",
      " 15.55532674 16.38751226 15.82133839 15.53909001 17.48927447 15.79629619\n",
      " 15.80356797 16.43595466 13.23379702 13.49635702 13.49465919 12.28499916\n",
      " 11.83022217 11.19162976 11.34359522 12.46639598 12.7648451  10.70093318\n",
      " 11.86206347 12.04943466 12.34882315 11.04734261 11.3315055  11.10999245\n",
      " 11.25013221 12.19887903 11.77816925 11.46359602 12.63875079 12.00600564\n",
      " 12.02943385 12.60642665 11.04980103 12.04758312 12.50867916 12.18313517\n",
      " 11.24307625 11.13454165 10.95790725 11.50432839 11.16584197 11.34598383\n",
      " 10.7403122  10.71226175 11.14323178 10.51158969 11.3587841  12.23078091\n",
      " 11.23736335 12.52356221 12.20265658 11.6318796  13.03566974 11.49455043\n",
      " 11.8270466  10.71473412 11.13562999 10.38214063 10.81105422 11.15388957\n",
      " 11.85443688 10.63920186 12.11800213 12.83153875 12.80390431 13.41276592\n",
      " 12.65452164 12.77871887 12.42465828 12.64740334 10.98661344 12.74708718\n",
      " 12.38814801 12.11272412 11.12432334 13.34884228 11.77086339 12.31173668]\n",
      "==============================================================\n",
      "SSE : 2.5222177227177967\n"
     ]
    }
   ],
   "source": [
    "def norm(u):\n",
    "    return np.sqrt(np.sum(u**2))\n",
    "\n",
    "def orth_proj(uj,ai):\n",
    "    return (np.dot(uj,ai) / (norm(uj)**2)) * uj\n",
    "\n",
    "Y_pred =0\n",
    "for i in range(0,q.shape[1]):\n",
    "    Y_pred += orth_proj(q[:,i],Y)\n",
    "print(\"predected values of Y: \", Y_pred)\n",
    "\n",
    "sse = np.sum((Y-Y_pred)**2)\n",
    "print(\"==============================================================\")\n",
    "print(\"SSE :\", sse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ce859a0-23be-42f2-b709-bf72b52d6e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights [-2.43833454e+01  1.79696033e+00  1.03270812e+01 -1.15968228e-01\n",
      "  9.45077486e-01  5.24042440e-03  3.00891684e-01]\n"
     ]
    }
   ],
   "source": [
    "Q,R = sub_proj_qr(X_train) \n",
    "DI = np.diag(1/(np.apply_along_axis(np.linalg.norm, 0, Q)**2))\n",
    "DQY = np.dot(DI, np.dot(Q.T, Y))\n",
    "\n",
    "# w using inverse\n",
    "w = np.dot(np.linalg.inv(R), DQY)\n",
    "print(\"weights\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c273f7a3-4b6f-47a1-bb7c-5732b814af83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Y values')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAk5UlEQVR4nO3de3iU9Zn/8fedkGAiEiSI9ZQECWpb8Rht66kQbLditWpP1uCJbhEQivvbtdste63aLte6bbeVBRHpFkFNtSc804MLeFiqVVAxWrWJEBAPIERRIEhI7t8fM0mHZJ7JJJlnZpL5vK6Ly8wz33nmnk33nm++z/3cX3N3REQkd+RlOgAREUkvJX4RkRyjxC8ikmOU+EVEcowSv4hIjhmU6QCSMWLECK+oqMh0GCIi/cratWu3ufshnY/3i8RfUVHBmjVrMh2GiEi/YmYb4x3XUo+ISI5R4hcRyTFK/CIiOUaJX0Qkxyjxi4jkGCV+EZEsUVtXS8UtFeTdlEfFLRXU1tWG8j79opxTRGSgq62rZcpDU9jdshuAjTs2MuWhKQDUjK1J6Xtpxi8ikgVmr5jdkfTb7W7ZzaRlk1I++1fiFxEJWTJLOJt2bAp8ffvsP1XJX4lfRCRE7Us4G3dsxPHAJF5WUpbwPLtbdjN7xeyUxKTELyISoqAlnM5JfM6EORTmFyY8V6K/CnpCiV9EJERBybrz8ZqxNRxUeFDCc3X3V0GylPhFREIUlKzjHW9qbgo8T3FBMXMmzElJTEr8IiJ90N2F2zkT5lBcULzfsaAkHvQlkW/5LLpgUcrKOlXHLyLSS9Mfmc7CNQtxHPhb9c3qTatZXr+cTTs2MbxoOHn2tzl2aVEpc8+bGzeJz5kwZ79afoh8SaQy6UOIM34zO8rMVpnZK2b2spnNih4fbmaPmll99L8HhxWDiEhYautq90v67Xa37GbhmoUdVTzbm7ezc+/Ojueb9zUHnrNmbA2LLlhEeUk5hlFeUp7ypA9g7t79qN6c2Oww4DB3f87MDgLWAhcBVwFN7n6zmX0XONjd/znRuaqqqlwbsYhINqm4pYKNO+Luc9Kt8pJyGq9rTG1AcZjZWnev6nw8tBm/u7/t7s9Ff/4QeAU4AvgSsDQ6bCmRLwMRkX6lL6WVqSrL7K20XNw1swrgZODPwKHu/jZEvhyAkQGvmWJma8xszbvvvpuOMEVEktaX0spUlWX2VuiJ38yGAL8FrnP3D5J9nbsvcvcqd6865JAuewWLiISitq6WET8cgd1k2E3GiB+OiNsqYc6EORjW4/Onsiyzt0JN/GZWQCTp17r7sujhLdH1//brAFvDjEFEclNvWhzX1tUy+YHJbG/e3nFse/N2rr7/6i6vrxlbw9SqqYHnyrd8DKO0qJTSotJQL9b2VGjlnGZmwM+BV9z9JzFPPQhcCdwc/e8DYcUgIrkpUYtjiLRR2LRjE2UlZcyZMKcjEc9eMZu9rXu7nK+lrYXZK2Z3SdgLzl8A0KW6J4wSzFQKs6rnLOBJoA5oix7+HpF1/l8BZcAm4KvuHny7GqrqEZHk1NbVMnvF7MBqm9KiUpr3Ne9XJ28YjlNeUp6wSscw2m5oi/tc+/vG+zLJpKCqntASfyop8YvkptiEOrxoOBBpaxAvubYv08SbsadCukowUyko8evOXRHJKrGz9vbZOLDfuvvGHRu5fNnlrN60mgXnL6C2rpYr7ruCNo8/I0+FTF+QTSUlfhHJCrV1tcz63az9Enznu2JjOc7CNQsBWLpuaahJv7SoNCuWblJFiV9EMq62rpar77+alraWHr3OcW5fe3uoSb+4oJi5580N7fyZoO6cIpIWicorZ/1uVo+Tfruwkn42lV+mmi7uikjoOpdXAhTkFTB08FCampsSLulkQn+8kBuPLu6KSNp0Lm/cuXdnl+0HW9pa9lvPzxbZcGdt2JT4RaRPOif5iWMmsnTd0v1unsqU0qLSLl8usXX77Qk+G2vww6SlHhHptXhLOLElmOkQ9H7lJeVA/C+egbKU0520t2UWkYGrvZHZpGWTuizh9CXpd9f0rLigmGlV0/bbqGRq1dTArQ2T3eg81yjxi0iPxGtklozSotKOWXiQ4UXDufuSu7sk8na7W3azvH45jdc10nZDG43XNbLg/AWBu1b1ZKPzXKLELyI9EtTILFbnmXt7LXzjdY0Jk39Tc1PH9oNB4s3Wa8bW7Pdl0L5G35ONznOJEr+I9Eh3yyTFBcVMrZpKaVFpx7GiQUUdPyfqY98+E68ZWxP4BdGT2Xq69rDtb5T4RQRIvn99osSbZ3ksumARZ5adud+m4tubtzPloSnU1tV29LGP91dB7Ew8VbP1oL8GcpkSv4h0VOds3LERxzv61wftPBXk4AMOpmZsDbNXzO5y0Xd3y25mr5gNRPrY33XJXQln4pqth0flnCJCxS0VgWWPcybM6VLnPmnZpMBzJeprn6invaSe+vGLSKC8m/KSLsMsLiimaFBRwqqeRLX1uVA/ny1Uxy8yQPVkb9mgsT25YNq+hBNUcgmRWv7u1vAlc5T4RfqxnqzNJxob70JqIk3NTR3r70Ha2yJofT77aKlHpB9LtDbfvqTS3T607ev41zx0DbtadiX1vrHnTyYGyQwt9YgMQN21JIid5QfZuGMjk5ZNSjrph1V2Kemj7pwi/VhZSVncpD68aHjgTLwvSotKmXve3C5ll5B7HS77My31iPRDQRuSAxTmF+Luvd7RqjPDlMz7KW3EItJPTX9kOovWLqLVW8m3fMZVjOOpzU91VNe0V9C0X0zduXdnyjY40Tr9wKQ1fpEs0rnc8tw7z+W2NbfR6q0AtHorKzasiNsKuT1JNzU3JXyP4oJi7r7k7m47ZRbkFWidfoBS4hcJUXvfervJsJuMET8cEVhnP/2R6Vy+7PL9yi1XbFiR9Htt3LGRilsqGF40PHBMbFllohLO0qJS7rjoDi3tDFBa6hFJgc7bD7bPlK++/+r91tq3N29n8gOTAfZLqrV1tSxcs7DPO1dt3LGRwvxCCvIK9nvf4oLiuL1wQBdlc5Eu7or0UdD2g8UFxYElkp3XzlNdgVNaVMqQwiFK6DlOF3dFQhKvE6XjCeviO9ffp3orwKbmJrZ9Z1tKzykDh9b4RbrRXS+c3iTtzr1xgnrlGEZhXmGfzy8SS4lfJIFkeuH0NMkW5hd2XANo/1KJt8xjGFOrprL4osU96qOju2alO0r8Igl0t6FIbV0t23YnXlI5sODAjp9Li0pZ/KXF1IytSdhOoT3pLzh/QdwNSaZVTet4XFpUSmlRqZqhSdJ0cVckRufqnEQXXKdVTWPpuqVdvhg6i1dRA91f0NXNU9JXatIm0o14yzpBm4ID3Lbmtm6TPuz/F0Ks7q4NpPqCr0g7JX7JKYku1AZV56RCvCTe3bUBXaCVsISW+M1ssZltNbOXYo7daGZvmtkL0X8Tw3p/yV1ByT3ejP7yZZcz/ZHpQLgz7HhJPNGds7pAK2EKs45/CTAfuLPT8Z+6+49DfF/JYZ1vpmqvwgGY9btZcWf0t625DQhucdxXQUk89s7ZjTs2km/5tHprx8YoukArYQn14q6ZVQAPu/vx0cc3Ajt7mvh1cTc3xWuD0F0yDLpgmmd5tHlb4OsMo3pUdY964yQjXv96kXTJpjt3Z5jZFcAa4B/d/b14g8xsCjAFoKxMa525JmjmvnrTapbXLw/8MgharkmU9CEy83+s8bGUxa9Zu2SzdM/4DwW2AQ78ADjM3Sd3dx7N+HNPopuaYi+4FuQVMHTwUJqamygrKUtpL/pkxP4lodm9ZJusmPG7+5aYgH4GPJzO95f+I2jm3rnKpqWtpSPRh7E+DzCkcAgf7fuoy45WhfmFHTdjifQnaU38ZnaYu78dfXgx8FKi8TLwBbUzNjOy4ebC4oJiFn5xIRC5ONz+JaPZvfRnoS31mNk9wDhgBLAFuCH6+CQiSz2NwDUxXwSBtNQzMNXW1XbpV59HHnl5eexr25eS9zCMooKipG606kzr9NLfpX2px92/Eefwz8N6P0m/3lTdxJr1u1ldlk/aaKOtLfGF2J5wnCtPvJLl9cu7lEwmuh6gdgkykKkfv/RKonr5oOTf+YsiXRdhl9cvj5vEa+tqmfzAZPa27t3vuPaalYFOLRukV5LpWhl79+z0R6Z3uWs2XYIuFNeMrWHxlxZTWlTacUx7zUouUHdO6ZW8m/Li9rExjLsuuSvuVoSp6nsTT/sSTjxatpFcpe6cklJBDcTyLC+wNUJfBXXKLC8pZ+nFSynIK+jyXOymJyISocQvvRLUYKzVW0NZuy8vKWdq1dQuyb+9D07N2BruuOiOLss2qrMX6UoXd6VHYi/QDi8a3qsyySDlJeVMHDORhWsW7vcXQmxyP7PszMBKopqxNUryIknQjD9HJepLn2xb41TO7PPIY86EOSw4fwF3XXLXftsMxu5eVTO2hsbrGmm7oY3G6xqV6EV6QRd3c1DnUkz42/aAQOBz7e2DU+3AggO5/YLblcRFUizo4q4Sfw4KaoBWXlIOxO95U15SHkrSV8WNSHiyokmbZEb7unzsnavxJNqBqn3/2VSWZGqXKZHMUOIfQOIl+NKiUj746IOO1ghBSR8iiXhE8YjAmX1fk/6gvEGUDC7paKGsPjgimaHE3491rrCJl+B7cgF2V8suygvKQ4k1z/JYctESJXqRLKCqnn4qXoVN54ZnvfGXbX/p9WtLi0qZVjWtS31/cUExd158p5K+SJZQ4s9iiUou4/XKyZTyknL8Bmfbd7ax4PwFLLpgUWA5pohknpZ6slR33S8TXYhNp3gXaHUjlUh204w/C9XW1XLlfVfG7X55zUPXUHFLRagNz5JVWlSq2bxIP6QZf5Zpn+kHVd/satnFrh270hzV/gxjatVUFpy/IKNxiEjvaMafAf1l7T6I4yyvX57pMESklzTjT7P+snafb/m0eVvgklK2xCkiPacZf5oF7Vw1adkkKm6pYHjR8AxF9jfFBcUsvXgpbTe0dbRx6CyoH7+IZD8l/jTrri1CuvahjZVneYHll/H67qvVgkj/1qOlHjM7GDjK3V8MKZ5+q/NG4u3tCDofH140PCPJPZE2bwtslNb+BRDUA19E+p9uu3Oa2WPAhUS+JF4A3gUed/f/F3Zw7bK9O2dtXS1X3391lztnJ4yawFObn+qytJNnebR5WzpDTEgdMkUGpr7suVvi7h8AlwB3uPupwLmpDjBbJNqEJOj45csuj9suYcWGFXErdLIp6WvZRiT3JDPjrwM+DywFZrv7s2b2orufkI4AIX0z/qANSq488UqWrlu63/HC/EIK8grY1ZLZmvrOSotKGVI4pGNZZuKYiSyvXx74WMs2IgNXX/rxfx/4A7A6mvSPBupTHWA2CKq4WbR2UZcbqva27mVv6950href0qJSmvc1d/mSmnveXCVyEUmo26Ued/+1u5/g7tOij9e7+5fDDy09YpdwgvrQJ+phnw7xqmrmnjdXzdBEpFe6nfGb2THAbcCh7n68mZ0AXOju/x56dCGLt7QTT6Jdq8JWWlTK3PPmBlbVKNGLSE8lc3H3Z8C/AC0A0VLOS8MMKhUStUVol0x7hOKCYsZVjAspysQK8go6lm4ar2uk7YZI2aWSvYj0RTKJv9jdn+l0bF8YwaRK501K2tsidE7+iW6mil0+aWhqCDXefMtnWtU07r7k7v2Wbu646A4leRFJuWQu7m4zs9EQadpiZl8B3g41qj4Kukg7e8Xs/RJpWUlZ3HX9znXtly+7PJQ4iwuKu6zLK9GLSNiSmfFfC9wOHGdmbwLXAdPCDKqvgmbynY8n246gp31piguKKS0qTThGvexFJFOSqepZ7+7nAocAx7n7We7eGHpkfRCUqDsfrxlbk1RlTNAXxLSqaR1NzPItH6DjHHPPm9vlNRBJ+HdfcjfbvrNNSV9EMiKZqp5/6/QYAHf/fjevWwx8Edjq7sdHjw0HfglUAI3A19z9vV7EndCcCXPi3ogV7w7VZLYJ7Eu/GvW4EZFsk8ydu/8Y8/AAIsn8FXef3M3rzgF2AnfGJP4fAk3ufrOZfRc42N3/ubsge3PnblDTNBGRXBF05263iT/OiQYDD7r73yUxtgJ4OCbxvwaMc/e3zeww4DF3P7a782R7kzYRkWzUlyZtnRUDR/cyjkPd/W2A6H9HBg00sylmtsbM1rz77ru9fDsREeksmTX+OujYfy+fyEXehOv7qeDui4BFEJnxh/1+IiK5Ipk6/i/G/LwP2OLuvb2Ba4uZHRaz1LO1l+cREZFeClzqMbPh0SqcD2P+NQNDo8d740HgyujPVwIP9PI8IiLSS4lm/GuJLPFYnOecbtb5zeweYBwwwsw2AzcANwO/MrNvApuAr/YiZhER6YPAxO/uo/pyYnf/RsBTE/pyXhER6ZukNluPbrI+hkgdPwDu/kRYQYmISHiSqer5e2AWcCSRzdY/DTwFVIcamYiIhCKZOv5ZwGnARncfD5wMqLBeRKSfSibx73H3PRC5a9fdXwW6vdtWRESyUzJr/JvNbBhwP/Comb0HvBVmUCIiEp5uE7+7Xxz98UYzWwWUAL8PNSoREQlNMhd35wK/dPc/ufvjaYhJRERClMwa/3PAv5pZg5n9yMy6dHoTEZH+I5kduJa6+0TgdOCvwH+aWX3okYmISCh60pa5EjiOyO5Zr4YSjYiIhK7bxG9m7TP87wMvAae6+wWhRyYiIqFIppxzA/AZd98WdjAiIhK+ZMo5F6YjEBERSY/ebL0oIiL9WKKNWJZHN0sXEZEBJNGMfwnwRzObbWYFaYpHRERClmgjll+Z2SPAvwFrzOwuoC3m+Z+kIT4REUmx7i7utgC7gMHAQcQkfhER6Z8CE7+ZfQH4CZEN0k9x991pi0pEREKTaMY/G/iqu7+crmBERCR8idb4z05nICIikh6q4xcRyTHJtGwQEZE02bbzI/7nyQ0sfPx1Dho8iCf/eTzDigtT+h5K/CIiGfRG025ue/x1fvHnTV2e+/CjfbS2ecrfU4lfRCSNXnvnQ25d1cCD64K3Lv/mWaOYcs7RHDr0gFBiUOIXEQnRc5veY/7KBla+ujXu8wX5xozxY7jqjApKitPTJEGJX0QkRdydJ+q3MW9FPWs2vhd3zMHFBcyoHsM3Tj+K4sLMpGAlfhGRXmptc3730tvMX9nAq+98GHfMUcOLmFk9hotPPoKC/OwopFTiFxFJ0t59bSx7bjPzVjbw5vvNccd88vChzKyu5POf+Bh5eZbmCJOjxC8iEmD33n3UPr2J/15Zz4d79sUd86lRw5lZPYYzK0sxy85E35kSv4hI1Hu79nLH6g3MW9WAB1RRnvvxQ5lRXclJRw1La2yppMQvIjnrnR17uP2J17ljdWPgmEtOPoLp40dTOfKg9AUWMiV+EckZ69/dya2rXue3z20OHHPFZ8q55rOjOWJYURojSy8lfhEZsOo272D+qnr+8PKWwDHXjh/N5DNHUTpkcBojy6yMJH4zawQ+BFqBfe5elYk4RGTgcHeeXt/EvJX1/On17XHHDBk8iBnVlUz6dDlDBufuvDeTn3y8u2/L4PuLSD/W1uY8+soW5q9soO7NHXHHfGzoAcyoruQrpx7JAQX5aY4we+XuV56I9Cv7Wtt44IW3mL+qgQ3bdsUdM2bkEGZOGMP5Yw8jP0tr6LNBphK/A380Mwdud/dFnQeY2RRgCkBZWVmawxORTNvT0sq9z2xi3soGtu/aG3fMKWXDmFk9hnHHHtJvauizQaYS/5nu/paZjQQeNbNX3f2J2AHRL4NFAFVVVanvSyoiWWVHcwt3/qmReasa2LuvLe6Yc445hBnjKzl91PA0RzewZCTxu/tb0f9uNbP7gNOBJxK/SkQGkq0f7uHnT27g9ifWB445/4TDmDG+ko8fNjSNkQ18aU/8ZnYgkOfuH0Z//jzw/XTHISLp9UbTbhY89jr3PNN1w5F23zj9KKZ+djTlpQemMbLck4kZ/6HAfdH1uEHAL9z99xmIQ0RC9Oo7HzB/ZQMPv/h24JhvnT2Kb519NCND2nBE4kt74nf39cCJ6X5fEQnX2o1NzFvZwGOvvRv3+cL8PGZWV3LFGRWUFKVnwxGJT+WcItJj7s5jf32XeSvqeW7T+3HHlB5YyIzqSi49rYyiQtXQZxMlfhHpVmub80jd28xfWc9ft+yMO6a8tJgZ4yu5KIs2HJH4lPhFpIuP9rXy27VvMn9lPW/t2BN3zPFHDGVm9Rg+9/FDs3bDEYlPiV9E2PXRPu5+eiPzVjaw86P4G4585uhSZlZX8pnR/WfDEYlPiV8kBzXt2svi/9vA/FUNgWM+94lDmTG+khP78YYjEp8Sv0gOeOv9ZhY9sZ4lf2oMHPPlU45k2rjRVI4ckr7AJCOU+EUGoIatO1mwqoFlz78ZOOaqMyqYcs7RHD6ANxyR+JT4RQaAFze/z7yVDTz6l+ANR2ZWV3L1maMYfmBhGiOTbKTEL9LPuDtPvb6deSsbeGp9/A1HDho8iJkTKrnsU7m94YjEp/9FiGS5tjbnj3/ZwryV9bz81gdxxxxecgAzqsfw5VOPYPAg3SwliSnxi2SZltY27n/+TeavamDj9t1xxxxz6BBmVo9hojYckV5Q4hfJsOa9rdz7bGTDkaaADUeqyg9mRnUlnz1GG45I3ynxi6TZjuYWlqxuZP6qelpa4+8xNO7YyIYjVRXacERST4lfJGRbP9jDz55cz8+e3BA45oITD+fa8aM57mPacETCp8QvkmKbtu/mtscbuOeZNwLHXPapMqaeM5qy0uI0RiYSocQv0kcvbn6f2x9fzyN1wRuOXHPO0Xzz7FGMPEgbjkjmKfGL9NADL7zJrHtfCHy+cFAe345uODL0AG04ItlHiV8kAXdn8epGfvDwXxKOu+nCT/L1047igALV0Ev2U+IXidHS2sZPHv0rtz32esJxf3/WKK7/wrG6WUr6JSV+yWm79+7jpgf/wi/XBF+IBfiX847jW2cfrQ1HZEBQ4pec8t6uvVz/m3X87ytbE4770VdO4CunHqmbpWRAUuKXAW3ze7u59hfPs+6N9xOO+58rqjj3E4emJyiRDFPilwHl1Xc+4Ft3ruGNpubAMcOKC/j5ladxavnBaYxMJHso8Uu/9syGJq5c/AzNLa2BY44+5EBun3QqYw49KI2RiWQvJX7pV37/0jtMvXttwjGnVRzM3EtP1s5SIgGU+CVruTv3PPMG37uvLuG4iWM/xn9cfAIlxbpZSiQZSvySNVrbnAWrGvivR/+acNykT5fxr+d/QjdLifSSEr9kzJ6WVv5j+SssfWpjwnHfnjCGb1dXMig/L02RiQxsSvySNh/saeF7y+p4+MXgZmYAP7joeCZ9qkw19CIhUeKX0Gz5YA+z7n2ep9c3JRx362WncP4Jh6UpKhFR4peUWf/uTq65ay31W3cGjinMz2PJ5NM4Y/SINEYmIrGU+KXX1r3xPpOXPMv2gH1iAY4YVsSiK07lk4eXpDEyEUlEiV+S9thrW7nqjmcTjhl7RAm3XnaKdpYSyWIZSfxm9gVgLpAP/I+735yJOCSx+57fzD/8cl3CMZ895hD+62snMmLI4DRFJSJ9lfbEb2b5wK3A54DNwLNm9qC7J97pQkLV1uYsXr2Bf3/klYTjvnLqkdx44ScZMlh/LIr0V5n4/97TgQZ3Xw9gZvcCXwKU+NOopbWNH//hNW5/Yn3CcdecczT/+PljKRykGnqRgSITif8IIHbXi83ApzoPMrMpwBSAsrKy9EQ2gO36aB83PPgyv1m7OeE4bTgiMvBlIvHHyyje5YD7ImARQFVVVZfnJbHtOz/i+t+8yMpXE2848pOvncglpxyZpqhEJBtkIvFvBo6KeXwk8FYG4hhQ3mjazbW/eI4XN+9IOO6Oq05j/HEj0xSViGSjTCT+Z4ExZjYKeBO4FLgsA3H0a5vf283Cx1/n7qc3BY4ZVlzA4qtO45QybTgiIn+T9sTv7vvMbAbwByLlnIvd/eV0x9Hf1G/5kFtXNXD/C8F/HI0ZOYTbJp1K5cghaYxMRPqbjNTkuftyYHkm3ru/eG7Te8xf2RC4Rp+fZ8wYX8nVZ1YwrLgwzdGJSH+mYuws4O48Wb+N+SsbeKYxfkOzkqICZlZXctmnyigu1K9NRHpPGSQD2tqc37/8Dv+9op5X3/kw7pijhhcxc/wYLjr5CNXQi0hKKfGnwd59bdz3/Gb+e0UDb77fHHfMxw8byszqSr7wyY+phl5EQqXEH4Lde/fxiz9vYt7KBnY0t8Qdc3rFcGZOqOSsyhHacERE0kqJPwXe372XO1Y3Mm9lPW0Bt5qd+/GRXDu+kpNVWikiGabE3wvv7NjD7U+8zh2rGwPHXHTS4UwfX8kxhx6UvsBERJKgxJ+EDdt2sWBVA79O0Ofm8k+Xc81nj+bIg9WHXkSymxJ/HC+9uYP5Kxv4/cvvBI6ZPm40k88apT70ItLvKPEDT6/fzryV9axu2B73+eLCfGZUV3L5p8s56ICCNEcnIpJaOZf43Z3/fWUr81fWsy6godnIgwYzc8IYvnrqkRxQkJ/mCEVEwjXgE/++1jYeevEt5q1oYP22XXHHVI4cwszqSs4fexiD8nWzlIgMbAM68a94ZQvfXLqmy/GTjhrGzOpKqo8bqRp6Eck5Azrxt194PXvMCGZWj+H0UcMzHJGISOYN6MR/4lHDaLz5/EyHISKSVbSgLSKSY5T4RURyjBK/iEiOUeIXEckxSvwiIjlGiV9EJMco8YuI5BglfhGRHGPuAVtGZREzexfYmOk4UmgEsC3TQWSIPntu0mfPjHJ3P6TzwX6R+AcaM1vj7lWZjiMT9Nn12XNNNn52LfWIiOQYJX4RkRyjxJ8ZizIdQAbps+cmffYsojV+EZEcoxm/iEiOUeIXEckxSvwhM7PFZrbVzF6KOfYDM3vRzF4wsz+a2eGZjDEs8T57zHP/ZGZuZiMyEVvYAn7vN5rZm9Hf+wtmNjGTMYYl6PduZjPN7DUze9nMfpip+MIU8Hv/ZczvvNHMXshgiIASfzosAb7Q6diP3P0Edz8JeBj4t3QHlSZL6PrZMbOjgM8Bm9IdUBotIc5nB37q7idF/y1Pc0zpsoROn93MxgNfAk5w908CP85AXOmwhE6f3d2/3v47B34LLMtAXPtR4g+Zuz8BNHU69kHMwwOBAXmFPd5nj/op8B0G6OeGhJ99wAv47NOAm939o+iYrWkPLA0S/d7NzICvAfekNag4lPgzxMzmmNkbQA0Dd8bfhZldCLzp7usyHUuGzIgu8y02s4MzHUwaHQOcbWZ/NrPHzey0TAeUAWcDW9y9PtOBKPFniLvPdvejgFpgRqbjSQczKwZmk0NfdJ3cBowGTgLeBv4ro9Gk1yDgYODTwPXAr6Iz4FzyDbJgtg9K/NngF8CXMx1EmowGRgHrzKwROBJ4zsw+ltGo0sTdt7h7q7u3AT8DTs90TGm0GVjmEc8AbUSal+UEMxsEXAL8MtOxgBJ/RpjZmJiHFwKvZiqWdHL3Oncf6e4V7l5BJBmc4u7vZDi0tDCzw2IeXgx0qXYawO4HqgHM7BigkNzq1nku8Kq7b850IBD580tCZGb3AOOAEWa2GbgBmGhmxxKZ9WwEpmYuwvDE++zu/vPMRpUeAb/3cWZ2EpGL2o3ANZmKL0wBn30xsDha5rgXuNIHYNuABP+bv5QsWeYBtWwQEck5WuoREckxSvwiIjlGiV9EJMco8YuI5BglfhGRHKPEL/2amX3bzF4xs1ozu9DMvtuD11aY2WXdjPkHM9tjZiUxx8aZ2Rkxjy8ys08kOMdUM7si+vNjZpb0xttmNszMpic7XiQZSvzS300HJrp7jbs/6O43dx4QvWsyngogYeIncpv9s0RuuGo3Djgj5vFFQNzEb2aD3H2hu9/ZzfsEGUbkM4qkjOr4pd8ys4XAZOA1IjcIvQdUufsMM1tCpEviycBzwIPA3OhLHTgHeBT4OLABWOruP+10/tHAQ0Q6S37P3f/OzCqAp4FW4F1gFnAfsCP678vAz4E/AWdG3/cgYKe7/9jMHgNeINKuYSgw2d2fMbMb28dE3/sl4IvAzUTaGb8GPOru15vZ9US6PA4G7nP3G/r4f0rJMbpzV/otd59qZl8Axrv7NjO7qtOQY4Bz3b3VzB4CrnX31WY2BNgDfBf4J3f/YsBbtDfVehI41sxGuntj9AsnNkk/CDzs7r+JPgYY5u6fjT6+sdN5D3T3M8zsHCJfWMcn+JjfBY6P9nLHzD4PjCHyxWHAg2Z2TrQdsEhStNQjA9mv3b01+vNq4Cdm9m0iSXlfEq+/FLg32lRtGfDVHrx3omZc90BH7/ahZjasB+f9fPTf80T+kjmOyBeBSNI045eBbFf7D+5+s5k9AkwEnjazcxO90MxOIJJQH43O4AuB9cCtPX3vODqvrzqwj/0nYgcEhQb8h7vfnmQcIl1oxi85wcxGR7uD/iewhshM+UMi6+/xfAO4sb2TqLsfDhxhZuVxXpfoPPF8PRrTWcAOd99BpGnbKdHjpxBpXx3v3H8AJkeXqzCzI8xsZA/eW0SJX3LGdWb2kpmtA5qB3wEvAvvMbJ2Z/UOn8ZcSuWgb677o8YeAi6ObZ58N3Atcb2bPRy8Id+c9M/sTsBD4ZvTYb4Hh0Y24pwF/BXD37cDqaOw/cvc/EtnD4SkzqwN+Q8++dERU1SMikms04xcRyTFK/CIiOUaJX0Qkxyjxi4jkGCV+EZEco8QvIpJjlPhFRHLM/wdBzQ6Emrs7mQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_train[:,1], Y,color='green')\n",
    "b = w[0]\n",
    "\n",
    "plt.plot(X_train[:,1], w[1]*X_train[:,1]+b)\n",
    "plt.xlabel(\"first Attribute\")\n",
    "plt.ylabel(\"Y values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491fb872-6bff-4651-b39d-d0c98d7bb5d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Q2. Logistic Regression: (Total 25 points)  For this question use all attributes as independent, except for the last one, which will be used as the caterogical target or class variable for multiclass logistic regression. \n",
    "\n",
    "##### a) (15 points) Show the weight vector for each class after exactly 4 iterations of **batch** gradient descent. That is, you may not use stochastic gradient descent, but rather you should used the batch gradient computed over all points (given on top of page 633) for each class $j$. Initialize the weight matrix $W$ as follows: \n",
    "\n",
    "[[0.37454012, 0.95071431, 0.73199394],\n",
    "   \n",
    "[0.59865848, 0.15601864, 0.15599452],\n",
    "\n",
    "[0.05808361, 0.86617615, 0.60111501],\n",
    "\n",
    "[0.70807258, 0.02058449, 0.96990985],\n",
    "\n",
    "[0.83244264, 0.21233911, 0.18182497],\n",
    "\n",
    "[0.18340451, 0.30424224, 0.52475643],\n",
    "\n",
    "[0.43194502, 0.29122914, 0.61185289],\n",
    "\n",
    "[0.13949386, 0.29214465, 0.36636184]]\n",
    "\n",
    "##### Where each column of $W$ gives the weight for the corresponding class. Use step size $\\eta = 10^{-5}$. You may use scipy.special.softmax.\n",
    "\n",
    "##### b) (5 points) Compute the cross entropy for the data after exactly 4 iterations.\n",
    "\n",
    "##### c) (5 points) What is the log-odds ratio for class 1 versus class 3 for attribute 1 after 4 iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "013f902a-bf5a-4e0f-8e06-0b7712e9ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wi = np.array([[0.37454012, 0.95071431, 0.73199394],\n",
    "\n",
    "[0.59865848, 0.15601864, 0.15599452],\n",
    "\n",
    "[0.05808361, 0.86617615, 0.60111501],\n",
    "\n",
    "[0.70807258, 0.02058449, 0.96990985],\n",
    "\n",
    "[0.83244264, 0.21233911, 0.18182497],\n",
    "\n",
    "[0.18340451, 0.30424224, 0.52475643],\n",
    "\n",
    "[0.43194502, 0.29122914, 0.61185289],\n",
    "\n",
    "[0.13949386, 0.29214465, 0.36636184]]).reshape(3,8)\n",
    "\n",
    "D = df.to_numpy()\n",
    "X_train = D[:,:7]\n",
    "n,d = X_train.shape\n",
    "X_train = np.hstack((np.ones([n,1]),X_train))\n",
    "Y_train = (pd.get_dummies(D[:,-1])).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b6a3f900-e864-46c0-b7da-2684da9f786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def MLR(X,Y,eta,ep,max_itr,weight):\n",
    "    n,d = X.shape\n",
    "    classes = Y.shape[1]\n",
    "    t=0\n",
    "    w = weight\n",
    "    diff = 1\n",
    "    while t<max_itr and diff>ep:\n",
    "        wp = w.copy()\n",
    "        ce = 0\n",
    "        for i in np.random.permutation(n):\n",
    "            pi = softmax(np.dot(w,X[i]))\n",
    "            ce += np.sum(Y[i] * pi)\n",
    "        for j in range(classes):\n",
    "            w_j = (Y[i,j] - pi[j]) * X[i,:]\n",
    "            w[j,:] = w[j,:] + eta * w_j\n",
    "        t += 1\n",
    "        diff = np.linalg.norm(w-wp)\n",
    "    print(\"log odds : \", pi[0])\n",
    "    print(t)\n",
    "    return w,ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "be3dfac4-e3e4-49f5-b3a4-a72547019a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log odds :  0.9997674845900448\n",
      "4\n",
      "weightvector :\n",
      "           0         1         2         3         4         5         6  \\\n",
      "0  0.374360  0.947970  0.729328  0.598503  0.154984  0.155407  0.057354   \n",
      "1  0.601225  0.709982  0.022319  0.970006  0.833111  0.212727  0.182188   \n",
      "2  0.304312  0.525591  0.432877  0.291288  0.612219  0.139693  0.292511   \n",
      "\n",
      "          7  \n",
      "0  0.865159  \n",
      "1  0.184062  \n",
      "2  0.366721  \n",
      "Cross entropy :\n",
      " 70.01417588025699\n"
     ]
    }
   ],
   "source": [
    "wt,ce = MLR(X_train, Y_train, 1e-5, 1e-4, 4,wi)\n",
    "print(\"weightvector :\\n\",pd.DataFrame(wt))\n",
    "print(\"Cross entropy :\\n\",ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694faece-3214-408a-bbd5-3f8ab535d352",
   "metadata": {},
   "source": [
    "#### Q3. SVM: (Total 25 points) CSCI4390 must use linear kernel, and CSCI6390 must use quadratic homogeneous kernel with $c=0$ for this question. Find the $\\mathbf{\\alpha}$ values via the stochastic GD method for SVM training. Use the last attribute as the class variable, but with +1 denoting class 1, and -1 denoting the other two classes. Set the random seed to 42, and initilize $\\mathbf{\\alpha}$ randomly. Use $C=1$, convergence thrshold $\\epsilon=0.001$ and maxiter=1000. Use hinge loss. Set step size as the inverse of the self-kernel value for each point. Answer the following questions:\n",
    "\n",
    "##### a) (10 points) Print the $\\mathbf{\\alpha}$ vector. How many non-zero values does $\\mathbf{\\alpha}$ have. What do those signify?\n",
    "\n",
    "##### b) (5 points) Find the weight vector $\\mathbf{w}$ and the bias $b$.\n",
    "\n",
    "##### c) (5 points) What is the signed distance of the point at index 70 in the data to the SVM hyperplane $h(\\mathbf{x})=0$ (counting from 0). In what region does the point lie: within or outside the margin, and correctly or incorrectly classified.\n",
    "\n",
    "##### d) (5 points) What is the effective margin of the hyperplane if we ignore incorrectly classified points. What would be the equation of the canonical hyperplane in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "74aeb231-9bf3-4efb-89f2-7c89a61233c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = df.to_numpy()\n",
    "X = D[:,:7]\n",
    "n,d = X.shape\n",
    "Y = D[:,-1]\n",
    "Y[Y<2] = int(1)\n",
    "Y[Y>1] = int(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6abe5c5a-f58f-44e4-a3d9-16392792677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "def SVM(K,y,c,e,itr):\n",
    "    one = np.ones(K.shape[0])\n",
    "    K1 = np.insert(K, -1, one, axis=1)\n",
    "\n",
    "    n = K.shape[0]\n",
    "    nta = np.ones(n)*1e-5\n",
    "    for i in range(n):\n",
    "        nta[i] = 1/K1[i][i]\n",
    "    t=0\n",
    "    alpha = np.random.rand(n)\n",
    "    while itr>=t:\n",
    "        alphaold = alpha.copy()\n",
    "        for k in range(n):\n",
    "\n",
    "            alpha[k] = alpha[k] + nta[k]*(1 - y[k]*(np.dot(alpha * y, K[:,k])))\n",
    "            if alpha[k]<0: \n",
    "                alpha[k]=0\n",
    "            elif alpha[k]>c: \n",
    "                alpha[k]=c\n",
    "        if np.linalg.norm(alpha - alphaold)<= e:\n",
    "            break\n",
    "        #alphaold = alpha\n",
    "        t += 1\n",
    "    print('t',t)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f6b6c36b-1e2a-4b3a-9859-f33cefe87d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t 1001\n"
     ]
    }
   ],
   "source": [
    "K = (np.dot(X, X.T))**2\n",
    "aph = SVM(K, Y,1,0.001,1000) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "65b0541f-9609-4986-9069-c0dea69d906a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non zero values in alpha:  40  the non zero values signify the point which lie on the support vector or the margin\n",
      "alpha:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.44871394 0.         0.\n",
      " 0.         0.52528658 0.         0.         0.         0.36953952\n",
      " 0.46036402 1.         1.         1.         1.         1.\n",
      " 1.         1.         0.92366294 0.         0.         0.\n",
      " 1.         1.         1.         1.         0.         0.\n",
      " 0.08029185 1.         1.         1.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.19525125 1.         0.         0.46337728 1.\n",
      " 0.         0.         0.         0.         0.         0.40485015\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.74050883 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         1.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         1.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.84152663 1.         0.40764626 1.\n",
      " 1.         1.         0.         1.         1.         0.\n",
      " 1.         1.         0.         0.         1.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"non zero values in alpha: \",np.sum(aph !=0),\" the non zero values signify the point which lie on the support vector or the margin\")\n",
    "print(\"alpha: \",aph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "02df7f7c-c1b2-46ac-9a01-63f4a9ced891",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsvm = np.dot(np.dot(aph,Y),X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1aa10dab-8abd-46f9-9195-e18c70424258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias : [-1.28047407 -1.21571446 -1.18357996 -1.17867393 -1.2694355  -1.21571446\n",
      " -1.28022877 -1.22650773 -1.44163719 -1.35725345 -1.30353242 -1.22675303\n",
      " -1.16223872 -1.19510913 -1.18357996 -1.17278669 -1.17278669 -1.2377916\n",
      " -1.14040689 -1.2054118  -1.2696808  -1.28022877 -1.24883017 -1.21694097\n",
      " -1.22675303 -1.3018153  -1.18357996 -1.19437323 -1.23582919 -1.25030198\n",
      " -1.24024462 -1.28243648 -1.29985289 -1.22945135 -1.31481629 -1.33517631\n",
      " -1.35578164 -1.34523368 -1.30230591 -1.22675303 -1.27017141 -1.2696808\n",
      " -1.17327729 -1.35602695 -1.27066201 -1.21694097 -1.25888753 -1.2694355\n",
      " -1.2537362  -1.31260857 -1.26183115 -1.25986874 -1.32364714 -1.28145528\n",
      " -1.34596958 -1.33419511 -1.237301   -1.24809427 -1.28096467 -1.10851769\n",
      " -1.13010422 -1.15365317 -1.12961362 -1.24809427 -1.17303199 -1.13010422\n",
      " -1.26330296 -1.25888753 -1.30009819 -1.24294293 -1.48652737 -1.44163719\n",
      " -1.42054125 -1.4911881  -1.43280633 -1.42152246 -1.45267576 -1.58244027\n",
      " -1.56060844 -1.40950268 -1.44237309 -1.44212779 -1.51768066 -1.5387766\n",
      " -1.51719006 -1.52013368 -1.49854714 -1.59396945 -1.52847393 -1.55055107\n",
      " -1.58194967 -1.48481026 -1.48481026 -1.44163719 -1.58170437 -1.46371432\n",
      " -1.53019104 -1.58293088 -1.5387766  -1.49560353 -1.37810409 -1.43182512\n",
      " -1.47401699 -1.52798333 -1.5081139  -1.44212779 -1.52086958 -1.45439287\n",
      " -1.60672513 -1.44580731 -1.42127716 -1.46322372 -1.45316636 -1.48481026\n",
      " -1.54932456 -1.51179343 -1.41048389 -1.51719006 -1.46346902 -1.55030577\n",
      " -1.51792597 -1.47450759 -1.39895472 -1.44850563 -1.26183115 -1.46984686\n",
      " -1.49683003 -1.45193985 -1.51719006 -1.38865205 -1.46248782 -1.4592989\n",
      " -1.42152246 -1.42152246 -1.43427814 -1.33419511 -1.46469553 -1.44212779\n",
      " -1.41097449 -1.45267576 -1.32340184 -1.33444041 -1.3018153  -1.28071937\n",
      " -1.27017141 -1.29396565 -1.25888753 -1.22699833 -1.30402302 -1.27409623\n",
      " -1.3018153  -1.29273915 -1.31481629 -1.22675303 -1.29102204 -1.28022877\n",
      " -1.22724363 -1.28047407 -1.30255121 -1.30255121 -1.34695079 -1.30206061\n",
      " -1.2377916  -1.2696808  -1.23852751 -1.24024462 -1.29273915 -1.30941965\n",
      " -1.25888753 -1.24809427 -1.26649188 -1.21571446 -1.22650773 -1.24833957\n",
      " -1.27188852 -1.24196173 -1.24907547 -1.21743157 -1.22699833 -1.27017141\n",
      " -1.18357996 -1.26256706 -1.26526537 -1.25962344 -1.30402302 -1.27115261\n",
      " -1.31285387 -1.21571446 -1.21595976 -1.17597561 -1.2375463  -1.22675303\n",
      " -1.25888753 -1.24833957 -1.22945135 -1.2054118  -1.21620506 -1.24883017\n",
      " -1.28317239 -1.18480647 -1.2375463  -1.16395584 -1.18431586 -1.22650773\n",
      " -1.22675303 -1.19461853 -1.22724363 -1.24024462 -1.237301   -1.24196173]\n",
      "weight vector : [[-3.74330159 -3.64027494 -0.21365765 -1.41367281 -0.81243872 -0.54481473]\n",
      " [-3.650087   -3.57404352 -0.21613519 -1.36240479 -0.81759005 -0.24971697]\n",
      " [-3.50535909 -3.45629878 -0.2219979  -1.29789048 -0.81857126 -0.66206887]\n",
      " ...\n",
      " [-3.23798041 -3.35081912 -0.21790136 -1.28439889 -0.7928146  -2.03968235]\n",
      " [-2.9043703  -3.24043342 -0.20902145 -1.2694355  -0.69567518 -0.88259496]\n",
      " [-3.01720902 -3.27232262 -0.21301986 -1.28611601 -0.7295268  -1.38276481]]\n"
     ]
    }
   ],
   "source": [
    "b = wsvm[:,-1]\n",
    "wt_svm = wsvm[:,:-1]\n",
    "print(\"bias :\", b)\n",
    "print(\"weight vector :\",wsvm[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c27d9f2d-5512-4ded-abed-d8922c439c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = (np.dot(wsvm.T,X))/np.linalg.norm(wt_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "520cd7ee-68d2-4ba2-9698-67d82afb7068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-148.04957545, -142.26329914,   -8.39220064,  -54.84620162,\n",
       "         -31.98326414,  -34.89102311,  -52.73554747],\n",
       "       [-142.26329914, -138.21423798,   -8.2133935 ,  -53.37056163,\n",
       "         -30.98888893,  -34.57296349,  -51.30014828],\n",
       "       [  -8.39220064,   -8.2133935 ,   -0.49109068,   -3.17366413,\n",
       "          -1.84031393,   -2.07715692,   -3.04866932],\n",
       "       [ -54.84620162,  -53.37056163,   -3.17366413,  -20.61908234,\n",
       "         -11.95682442,  -13.39833868,  -19.82080174],\n",
       "       [ -31.98326414,  -30.98888893,   -1.84031393,  -11.95682442,\n",
       "          -6.96050925,   -7.7051419 ,  -11.48895331],\n",
       "       [ -34.89102311,  -34.57296349,   -2.07715692,  -13.39833868,\n",
       "          -7.7051419 ,  -10.31182264,  -12.93896893],\n",
       "       [ -52.73554747,  -51.30014828,   -3.04866932,  -19.82080174,\n",
       "         -11.48895331,  -12.93896893,  -19.07430926]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"hyperplane is given by :\",r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "efd26443-484f-4ab2-994b-a9c9bb3ac06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance of dist 70th point from hyperplane 7039.277495744218\n"
     ]
    }
   ],
   "source": [
    "dist = Y[69]*np.linalg.norm(np.dot(X[69,:],r))\n",
    "print(\"distance of dist 70th point from hyperplane\", dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cc532-2a82-4b72-86f7-b66434c2d21f",
   "metadata": {},
   "source": [
    "The point is correctly classified because the distance is positive and the point is outside the margin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f65aae4-d878-414a-93f7-2dc7095270d1",
   "metadata": {},
   "source": [
    "the margin of the hyperplan is 1 and the equation of the canonical hyperplane is 1/||W||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30951748-a587-42ff-b8a8-e8a337f55f2b",
   "metadata": {},
   "source": [
    "#### Q4. Neural Networks: (Total 25 points) Implement a one hidden layer MLP for the seeds dataset, where the target is the class variable. That is, you must use softmax activation for the output layer, with cross-entropy (CE) loss.  Use stochastic GD. Note that for the output layer, the net gradient $\\mathbf{\\delta}_o = \\mathbf{o} - \\mathbf{y}$ for softmax and CE loss (see eq. 25.52). However, for the hidden layer, the net gradient $\\delta_h$ is the derivative of the ReLU function multiplied elementwise with $(\\mathbf{W}_o \\mathbf{\\delta}_o)$.  Answer the following questions:\n",
    "\n",
    "##### a) (15 points) Implement the MLP model with one hidden layer. Set the random seed to 42, and initialize the weight matrices and bias vectors randomly. For the hidden layer use $m=16$ neurons, with ReLU activation. Use $\\eta=0.001$. Train for 200 iterations. Print the parameters of the model: $\\mathbf{b}_h, \\mathbf{W}_h, \\mathbf{b}_o, \\mathbf{W}_o$ after training.\n",
    "\n",
    "##### b) (5 points) What is the cross-entropy and accuracy of your model.\n",
    "\n",
    "##### c) (5 points) i) For the first point in the data (at index 0), what is the hidden layer value, and output vector (after model has been trained)? ii) What is the CE loss for the first point? iii) Print the net gradient vectors at the output and hidden layer for the first point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "65483bca-287a-4dcc-b0b1-c21920305e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = df.to_numpy()\n",
    "X = D[:,:7]\n",
    "n,d = X.shape\n",
    "Y = D[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ecc6ff53-d009-4b64-a91e-4dc118c0022a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3333333333333333\n",
      "𝐛ℎ:  [-0.22125362 -0.2768133   0.3074067   0.81573721  0.86047349 -0.58307744\n",
      " -0.16712171  0.28257995 -0.24869113  1.60734558  0.49097495  0.73487779\n",
      "  0.66288127  1.17347386  0.18102156 -1.29683195]\n",
      "𝐖ℎ:  [[-1.06511366 -0.3052247  -0.6095122  -0.1869713   0.05664992  0.52969275\n",
      "  -0.07049878  0.48650164  0.06447441 -1.97546657 -0.93933539 -0.14408756\n",
      "  -1.20969474  0.59992873  1.53075083  1.21876185]\n",
      " [-0.21344287  1.49072614  0.14866746 -0.33708597 -0.61340266 -0.30246969\n",
      "  -0.38817682  0.17041622  0.16057398  0.00304602  0.43693817  1.19064627\n",
      "   0.94955414 -1.48489797 -2.55392113  0.93431991]\n",
      " [-1.3668787  -0.2247654  -1.17011303 -1.80198044  0.54146273  0.75915516\n",
      "  -0.5765104  -2.59104229 -0.54624445  0.39180401 -1.47891157  0.18335992\n",
      "  -0.01530985  0.5792915   0.11958037 -0.97306894]\n",
      " [ 1.1965715  -0.15852957 -0.02730454 -0.93326796 -0.44328225 -0.88480271\n",
      "  -0.17294606  1.71170848 -1.37190114 -1.6135614   1.47117033 -0.20932368\n",
      "  -0.66907274  1.03990469 -0.60561554  1.82600971]\n",
      " [ 0.67792587 -0.48791141  2.15730821 -0.60571492  0.74209537  0.29929258\n",
      "   1.30174129  1.5615112   0.03200415 -0.75341787  0.45997214 -0.67771537\n",
      "   2.01338725  0.13653533 -0.36532155  0.18468031]\n",
      " [-1.34712629 -0.97161404  1.20041391 -0.65689428 -1.04691098  0.53665275\n",
      "   1.18570415  0.71895331  0.99604769 -0.75679509 -1.42181067  1.50133365\n",
      "  -0.32267984 -0.25083302  1.32819414  0.55623001]\n",
      " [ 0.45588777  2.16500234 -0.64351823  0.92784013  0.05701312  0.26859228\n",
      "   1.52846843  0.50783576  0.53829608  1.07250734 -0.36495273 -0.83920967\n",
      "  -1.04480919 -1.96635659  2.05620713 -1.10320837]]\n",
      "𝐛𝑜:  [-0.13845598]\n",
      "𝐖𝑜:  [[ 0.39968795]\n",
      " [-0.65135689]\n",
      " [-0.52861668]\n",
      " [ 0.58636402]\n",
      " [ 1.23828307]\n",
      " [ 0.02127158]\n",
      " [ 0.30883301]\n",
      " [ 1.70221494]\n",
      " [ 0.24075318]\n",
      " [ 2.60168311]\n",
      " [ 0.56550965]\n",
      " [-1.76076276]\n",
      " [ 0.75334162]\n",
      " [ 0.38115838]\n",
      " [ 1.28975275]\n",
      " [ 0.67318135]]\n",
      "cross_entropy_loss:  [inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/hwvzzncj2f9fs4sfs8f7q2vh0000gn/T/ipykernel_44354/2614069309.py:62: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.log(1 - yHat)\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "n,d = X.shape\n",
    "m=16\n",
    "\n",
    "W_h = np.random.randn(d, m)\n",
    "b_h = np.random.randn(m)\n",
    "W_out = np.random.randn(m, 1)\n",
    "b_out = np.random.randn(1)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def f_i(x):\n",
    "    return relu(x)\n",
    "\n",
    "def f_o(x):\n",
    "    return softmax(x)\n",
    "\n",
    "eta = 0.001\n",
    "\n",
    "for i in range(200):\n",
    "    # forward pass\n",
    "    xi = X[i,:]\n",
    "    net_zi = np.dot(xi, W_h) + b_h\n",
    "    zi = f_i(net_zi)\n",
    "\n",
    "    net_oi = np.dot(zi, W_out) + b_out\n",
    "    oi = f_o(net_oi)\n",
    "\n",
    "    # back propagation\n",
    "    yi = Y[i]\n",
    "    delta_o = (oi - yi) * oi * (1 - oi)\n",
    "    delta_h = np.dot(delta_o, W_out.T) * zi * (1 - zi)\n",
    "\n",
    "    W_out -= eta * np.outer(zi, delta_o)\n",
    "    b_out -= eta * delta_o\n",
    "\n",
    "    W_h -= eta * np.outer(xi, delta_h)\n",
    "    b_h -= eta * delta_h\n",
    "\n",
    "    # evaluate on validation set\n",
    "    #net_z = np.dot(valid, W_h) + b_h\n",
    "    #z = f_i(net_z)\n",
    "\n",
    "    #net_o = np.dot(z, W_out) + b_out\n",
    "    #o = f_o(net_o)\n",
    "\n",
    "    # compute accuracy\n",
    "    pred = np.where(oi > 0.5, 1, 0)\n",
    "    acc = np.mean(pred == Y)\n",
    "print('Accuracy: ', acc)\n",
    "print(\"𝐛ℎ: \",b_h)\n",
    "print(\"𝐖ℎ: \", W_h)\n",
    "print(\"𝐛𝑜: \",b_out)\n",
    "print(\"𝐖𝑜: \",W_out)\n",
    "def cross_entropy_loss(yHat, y):\n",
    "        if y[i] == 1:\n",
    "          return -np.log(yHat)\n",
    "        else:\n",
    "          return -np.log(1 - yHat)\n",
    "print(\"cross_entropy_loss: \", cross_entropy_loss(pred, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "891015c2-c795-4843-be53-12416a5bdb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
